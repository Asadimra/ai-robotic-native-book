---
id: how-vlas-work-from-pixels-to-actions
title: "How VLAs work: From Pixels to Actions"
sidebar_label: "How VLAs work: From Pixels to Actions"
---

## Learning Objectives

After completing this lesson, you should be able to:
- Identify the common inputs for a Vision-Language-Action (VLA) model.
- Describe the typical outputs generated by a VLA model to control a robot.
- Understand how these inputs and outputs are structured as data.

## From Pixels and Prompts to Physical Actions

At a high level, VLA models are functions that map sensory inputs and language commands to robot actions. Let's look more closely at what these inputs and outputs are.

## VLA Model Inputs

The model needs rich information about both the world and the user's intent.

### 1. Vision Inputs

The primary way a VLA perceives the world is through its "eyes."
-   **RGB Camera Images:** Standard color images provide rich texture, color, and object information. This is crucial for identifying what an object is.
-   **Depth Images:** Often captured by depth sensors (like LiDAR or Time-of-Flight cameras), these images encode the distance of each pixel from the camera. This provides critical 3D information about the scene, helping the robot understand object shapes, sizes, and spatial relationships.
-   **Multiple Camera Views:** Some robots use multiple cameras (e.g., one on the wrist, one on the head) to get different perspectives, which can help resolve ambiguities and provide a more comprehensive view of the workspace.

### 2. Language Inputs

The user's command is given as a natural language text prompt.
-   **Text Prompt:** A string of text like `"pick up the green block"` or `"push the blue objects to the left side."` This prompt is converted into a numerical representation (an "embedding") that the neural network can process.

### 3. Robot State Inputs (Proprioception)

The model often needs to know the current state of the robot's own body. This is known as **proprioception**.
-   **Joint Angles:** The current angle of each of the robot's joints.
-   **Gripper State:** Whether the gripper is open or closed, and how wide.
-   **End-Effector Pose:** The position and orientation of the robot's "hand" in 3D space.

```
+------------------------------------------------+
|                   VLA INPUTS                   |
|------------------------------------------------|
|                                                |
| +----------------+   +-----------------------+ |
| | Vision         |   | Language              | |
| | - RGB Image(s) |   | - "Pick up the apple" | |
| | - Depth Image  |   +-----------------------+ |
| +----------------+   +-----------------------+ |
|                      | Robot State           | |
|                      | - Joint Angles        | |
|                      | - Gripper Open/Closed | |
|                      +-----------------------+ |
|                                                |
+------------------------------------------------+
```

## VLA Model Outputs

After processing the inputs, the VLA model generates a sequence of actions for the robot to execute. The exact format of the output can vary, but it typically defines a trajectory for the robot's end-effector.

### 1. Action Sequences

The output is not just a single action, but a plan or a sequence of actions over a short time horizon. This is crucial for smooth and coordinated movement.

### 2. Trajectories

The actions often define a **trajectory**, which is a path for the robot's end-effector to follow through 3D space. This can be represented in several ways:
-   **End-Effector Pose Commands:** A sequence of target positions and orientations for the robot's gripper.
-   **Joint Angle Commands:** A sequence of target angles for each of the robot's joints.
-   **Velocity Commands:** A sequence of velocities for the robot's end-effector or joints.

The robot's low-level controller then takes these commands and executes them.

```
+------------------------------------------------+
|                  VLA OUTPUTS                   |
|------------------------------------------------|
|                                                |
|           Action Sequence (Trajectory)         |
|                                                |
| +--------------------------------------------+ |
| | Timestep 1: Move gripper to (x1, y1, z1)   | |
| | Timestep 2: Move gripper to (x2, y2, z2)   | |
| | Timestep 3: Close gripper                  | |
| | ...                                        | |
| +--------------------------------------------+ |
|                                                |
+------------------------------------------------+
```

## Summary

In this lesson, we unpacked the data flow for Vision-Language-Action models. We learned that VLAs take in a rich set of **inputs**, including **visual data** (RGB and depth images), **language commands**, and the robot's own **proprioceptive state**. Based on these inputs, they generate a sequence of **actions** as **outputs**, typically defining a trajectory for the robot's end-effector to follow. This mapping from multi-modal sensory input to concrete physical action is the core mechanism that allows VLAs to perform a wide range of tasks in the real world.
